# from openai import OpenAI
# from llama_index.readers.file import PDFReader
# from llama_index.core.node_parser import SentenceSplitter
# from dotenv import load_dotenv

# load_dotenv()

# client = OpenAI()
# EMBED_MODEL = "text-embedding-3-large"
# EMBED_DIM = 3072

# splitter = SentenceSplitter(chunk_size=1000, chunk_overlap=200)


# def load_and_chunk_pdf(path: str):
#     docs = PDFReader().load_data(file=path)
#     texts = [d.text for d in docs if getattr(d, "text", None)]
#     chunks = []
#     for t in texts:
#         chunks.extend(splitter.split_text(t))
#     return chunks
# def embed_texts(texts: list[str]) -> list[list[float]]:
#     response = client.embeddings.create(
#         model=EMBED_MODEL,
#         input=texts,
#     )
#     return [item.embedding for item in response.data]
from llama_index.readers.file import PDFReader
from llama_index.core.node_parser import SentenceSplitter
from sentence_transformers import SentenceTransformer

splitter = SentenceSplitter(chunk_size=1000, chunk_overlap=200)

# Load model multilingual offline
MODEL_NAME = "Alibaba-NLP/gte-multilingual-base"
EMBED_DIM = 768 
model = SentenceTransformer(
    MODEL_NAME,
    trust_remote_code=True 
)

def load_and_chunk_pdf(path: str):
    docs = PDFReader().load_data(file=path)
    texts = [d.text for d in docs if getattr(d, "text", None)]
    chunks = []
    for t in texts:
        chunks.extend(splitter.split_text(t))
    return chunks

def embed_texts(texts: list[str], batch_size: int = 32) -> list[list[float]]:
    embeddings = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        batch_emb = model.encode(batch, show_progress_bar=False)
        embeddings.extend(batch_emb)
    return embeddings
